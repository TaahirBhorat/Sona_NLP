---
title: "Sona_NLP"
---

Install packages if necessary and load them

```{r}
#install.packages(c("tm", "textTinyR", "word2vec", "data.table", "caret", "keras", "tidytext"))
library(tm)
library(textTinyR)
library(word2vec)
library(data.table)
library(caret)
library(keras)
library(data.table)
library(ggplot2)
library(stringr)
library(dplyr)
library(tidytext)

```

# Introduction

blah blah blah

# Data Cleaning

```{r, warning=FALSE, message=FALSE}
# Install and load the stringi packagedf
#install.packages("stringi")
library(stringi)
# Function to parse text document into sentences excluding the first line
parse_sentences <- function(file_path) {
  # Read the text and exclude the first line
  text_data <- readLines(file_path)
  text_without_first_line <- paste(text_data[-1], collapse = " ")
  
  # Split the text into sentences and remove empty or whitespace-only strings
  sentences <- unlist(stri_split_boundaries(text_without_first_line, type = "sentence"))
  sentences <- sentences[stri_trim_both(sentences) != ""]
  
  return(sentences)
}

# Initialize an empty dataframe
df <- data.frame(Year = integer(), President = character(), Sentences = character())

# List of provided files
files <- c(
  "2023_Ramaphosa.txt", "2022_Ramaphosa.txt", "2021_Ramaphosa.txt", "2020_Ramaphosa.txt", "2019_Ramaphosa.txt", 
  "2019_Ramaphosa_2.txt", "2018_Ramaphosa.txt", "2017_Zuma.txt", "2016_Zuma.txt", "2015_Zuma.txt", 
  "2014_Zuma.txt", "2014_Zuma_2.txt", "2013_Zuma.txt", "2012_Zuma.txt", "2011_Zuma.txt", "2010_Zuma.txt", 
  "2009_Zuma.txt", "2009_ Motlanthe.txt", "2008_Mbeki.txt", "2007_Mbeki.txt", "2006_Mbeki.txt", 
  "2005_Mbeki.txt", "2004_Mbeki.txt", "2004_Mbeki_2.txt", "2003_Mbeki.txt", "2002_Mbeki.txt", 
  "2001_Mbeki.txt", "2000_Mbeki.txt", "1999_Mandela.txt", "1999_Mandela_2.txt", "1998_Mandela.txt", 
  "1997_Mandela.txt", "1996_Mandela.txt", "1995_Mandela.txt", "1994_Mandela.txt", "1994_deKlerk.txt"
)

# Loop through each file and append to dataframe
for (file in files) {
  # Extract year and president's name from filename
  file_info <- strsplit(file, "_")[[1]]
  year <- as.integer(file_info[1])
  president_name <- gsub(".txt", "", file_info[2])
  
  sentences <- parse_sentences(file)
  
  temp_df <- data.frame(Year = year, President = president_name, Sentences = sentences)
  df <- rbind(df, temp_df)
  data= df
}

```

# EDA 

## Sentences per President

```{r, echo=FALSE}
# Summary statistics for the 'President' column
barplot(table(data$President), las = 2, main = "Number of Sentences per President", col = rainbow(length(unique(data$President))))


```

## Average Sentence Length per President

```{r, echo=FALSE}
# Average sentence length by president
data$word_count <- str_count(data$Sentences, "\\w+")
avg_sentence_length <- data %>% group_by(President) %>% summarize(avg_length = mean(word_count))

ggplot(avg_sentence_length, aes(x = President, y = avg_length)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(title="Average Sentence Length per President", y="Average Number of Words")





```

## Top words words overall

```{r, echo=FALSE}
# Visualization of top 10 words
data_tidy <- data %>%
  unnest_tokens(word, Sentences)

word_freq <- data_tidy %>%
  count(word, sort = TRUE)
ggplot(head(word_freq, 10), aes(x = reorder(word, n), y = n)) + 
  geom_bar(stat="identity") + 
  coord_flip() + 
  labs(title = "Top 10 Most Frequent Words", x = "Words", y = "Frequency")
```

# Feature Extraction

## TF-IDF Tokenisation 

Here I am running the TF-IDF tokenisation then splitting the data into training, validation and test sets

```{r, message=FALSE, warning=FALSE}
corpus <- Corpus(VectorSource(data$Sentences))
dtm <- DocumentTermMatrix(corpus, control = list(weighting = weightTfIdf))

X_tfidf <- as.matrix(dtm)
setDT(data)
setkey(data, President)
train_indices <- createDataPartition(data$President, p = 0.6, list = FALSE)
X_train_tfidf <- X_tfidf[train_indices, ]
y_train_tfidf <- data$President[train_indices]
X_temp_tfidf <- X_tfidf[-train_indices, ]
y_temp_tfidf <- data$President[-train_indices]
val_indices <- createDataPartition(y_temp_tfidf, p = 0.5, list = FALSE)
X_val_tfidf <- X_temp_tfidf[val_indices, ]
y_val_tfidf <- y_temp_tfidf[val_indices]
X_test_tfidf <- X_temp_tfidf[-val_indices, ]
y_test_tfidf <- y_temp_tfidf[-val_indices]

```
